# -*- coding: utf-8 -*-
"""Face Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O6UwYPQ68PO1EpKLRDlzVRyoe-uOR-8W
"""

import numpy as np
import matplotlib
import imageio
from numpy import linalg as LA
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from matplotlib import pyplot as plt
import os
from sklearn.model_selection import train_test_split

"""1. Download the Dataset and Understand the Format (10 Points)"""

#This cell is responsible for connecting the notebook with my google drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

"""2. Generate the Data Matrix and the Label vector (10 Points)"""

def Generate(path, folder_num, image_num, images_total):
#path = "/content/drive/MyDrive/ORL_dataset"
  sub_id = 1
  img_num = 1
  d = np.zeros((0, 10304), int)       #Data Matrix (D) 
  label_y= np.zeros(images_total, int)         #Label Vector (label_y)
  for i in range(folder_num):
          sub_id = i + 1
          for j in range(image_num):
              img_num = j + 1
              #print(path+str(sub_id)+'/'+str(img_num)+'.pgm')
              imgvctr = imageio.imread(path+str(sub_id)+'/'+str(img_num)+'.pgm')
              x=np.array(imgvctr)
              flat = x.flatten()
              d = np.append(d, np.array([flat]), axis=0)
              label_y[i*image_num+j]=sub_id
  print("Data Matrix: 400x10304\n",d)
  print("\nY label vector: \n",label_y)
  return d, label_y

d, label_y = Generate("/content/drive/MyDrive/ORL_dataset/s",40,10,400)

"""3. Split the Dataset into Training and Test sets (10 Points)"""

def Split(d, label_y,image_num,even_odd,percentage_test):
  images_total=len(d)
  print(images_total)
  d_train = np.empty((0, 10304), int)
  d_test = np.empty((0, 10304), int)
  y_train = np.array([])
  y_test = np.array([])
  test_subjectimages=0
  for i in range(images_total):
    if (even_odd==1):#If the user wants to split the dataset on even and odd images
      test_subjectimages=5 #images per subject
      if i % 2 == 0:
          d_test = np.append(d_test, [d[i]], axis=0)
          y_test = np.append(y_test, [label_y[i]], axis=0)
      else :
          d_train = np.append(d_train, [d[i]], axis=0)
          y_train = np.append(y_train, [label_y[i]], axis=0)
    else :#if the user wants to split D into any percentage
      count=i%image_num #image num is number of images per subject
      test_subjectimages=percentage_test*image_num
      if (count<percentage_test*image_num):
          #print(count)
          d_train = np.append(d_train, [d[i]], axis=0)
          y_train = np.append(y_train, [label_y[i]], axis=0)
      else:
          d_test = np.append(d_test, [d[i]], axis=0)
          y_test = np.append(y_test, [label_y[i]], axis=0)
  print("Training Dataset: \n",d_train)
  print("Dimensions: ",d_train.shape)
  print("\nTesting Dataset: \n",d_test)
  print("Dimensions: ",d_test.shape)
  return d_test, y_test, d_train, y_train,test_subjectimages

d_test, y_test, d_train, y_train,test_subjectimages = Split(d,label_y,10,1,0.5)

"""4. Classification using PCA (30 points)"""

def PCA(d_test, y_test, d_train, y_train, images_total):
  mean_vector = np.mean(d_train,axis=0)
  z_train = d_train-mean_vector
  z_test = d_test-mean_vector
  cov = 1/(images_total/2)*np.dot(z_train.T,z_train)
  eigen_values, eigen_vectors = LA.eigh(cov)
  eigen_diag = np.diag(eigen_values)
#A copy before fliping
  eigen_vectors_bff = eigen_vectors
#Gonna have those duplicates in order to save runtime if sth happened i'll have a backup of eigen val and vec
  eigen_vectors_save = eigen_vectors
  eigen_values_save = eigen_values
#sort the array in place in descending order
  eigen_values[::-1].sort() 
  eigen_values_sum = np.sum(eigen_values)
  eigen_vectors = np.flip(eigen_vectors_save,axis=1)
  print("Eigen Values Sum",eigen_values_sum)
  print("Eigen Vectors: \n",eigen_vectors)
  x = 0
  r1 = 0
  r2 = 0
  r3 = 0
  r4 = 0
  sum = 0
  for x in range(np.size(eigen_values,0)):
    sum = sum + eigen_values[x]
  
    if (sum/eigen_values_sum>=0.8 and r1==0):
      r1=x
    elif (sum/eigen_values_sum >= 0.85 and r2==0):
      r2=x
    elif (sum/eigen_values_sum>=0.9 and r3==0):
      r3=x
    elif (sum/eigen_values_sum>=0.95 and r4==0):
      r4=x
      break
#Computing the projection vectors for alpha = 0.8
  print("Number of principle compenents is: ",r1+1)
  projection_U1 = np.zeros((10304,r1+1))
  projection_U1 = eigen_vectors[:,:r1+1]
 # print("Projection Matrix U1: \n",projection_U1)
  projected_data_u1 = np.dot(d_train,projection_U1)
  projected_data_test_u1 = np.dot(d_test,projection_U1)
#Computing the projection vectors for alpha = 0.85
  print("Number of principle compenents is: ",r2+1)
  projection_U2 = np.zeros((10304,r2+1))
  projection_U2 = eigen_vectors[:,:r2+1]
 # print("Projection Matrix U2: \n",projection_U2)
  projected_data_u2 = np.dot(d_train,projection_U2)
  projected_data_test_u2 = np.dot(d_test,projection_U2)
#Computing the projection vectors for alpha = 0.9
  print("Number of principle compenents is: ",r3+1)
  projection_U3 = np.zeros((10304,r3+1))
  projection_U3 = eigen_vectors[:,:r3+1]
 # print("Projection Matrix U3: \n",projection_U3)
  projected_data_u3 = np.dot(d_train,projection_U3)
  projected_data_test_u3 = np.dot(d_test,projection_U3)
#Computing the projection vectors for alpha = 0.95
  print("Number of principle compenents is: ",r4+1)
  projection_U4 = np.zeros((10304,r4+1))
  projection_U4 = eigen_vectors[:,:r4+1]
 # print("Projection Matrix U4: \n",projection_U4)
  projected_data_u4 = np.dot(d_train,projection_U4)
  projected_data_test_u4 = np.dot(d_test,projection_U4)
  return mean_vector, projected_data_u1, projected_data_u2, projected_data_u3, projected_data_u4, projected_data_test_u1, projected_data_test_u2, projected_data_test_u3, projected_data_test_u4

mean_vector, projected_data_u1, projected_data_u2, projected_data_u3, projected_data_u4, projected_data_test_u1, projected_data_test_u2, projected_data_test_u3, projected_data_test_u4 = PCA(d_test, y_test, d_train, y_train,400)

"""c. Use a simple classifier (first Nearest Neighbor to determine the class labels). d. Report Accuracy for every value of alpha separately.

"""

def KNN(n_neighbors, projected_data, projected_data_test, y_train, y_test):
  knn = KNeighborsClassifier(n_neighbors)
  knn.fit(projected_data, y_train)
  y_pred = knn.predict(projected_data_test)
  print(y_pred)
  print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
  return metrics.accuracy_score(y_test, y_pred)

KNN(1,projected_data_u1,projected_data_test_u1, y_train, y_test)
KNN(1,projected_data_u2,projected_data_test_u2, y_train, y_test)
KNN(1,projected_data_u3,projected_data_test_u3, y_train, y_test)
KNN(1,projected_data_u4,projected_data_test_u4, y_train, y_test)

"""6. Classifier Tuning (20 Points) for PCA"""

#for alpha=  0.95
accuracy_arr=np.zeros(4)
k_values=np.array([1,3,5,7])
for i in range(4):
  knn = KNeighborsClassifier(n_neighbors=k_values[i])
  knn.fit(projected_data_u4, y_train)
  y_pred = knn.predict(projected_data_test_u4)
  #print(y_pred)
  accuracy_arr[i]=metrics.accuracy_score(y_test, y_pred)
  print("Accuracy:",accuracy_arr[i])

plt.plot(k_values,accuracy_arr)
plt.xlabel("k values")
plt.ylabel("accuracy")
plt.title('Performance Measure')
plt.show()

"""5. Classification Using LDA (30 Points)"""

def LDA(d_test, d_train,test_subjectimages):
  classmeans=np.empty((0, 10304), float)
  subject = np.empty((0, 10304), float)
  z_train = np.empty((0, 10304), float)
  s=np.zeros((10304,10304))#zT.z is a DxD matrix
  sb =np.zeros((10304,10304))
  print(test_subjectimages)
  for i in range (len(d_train)):
    image_no=i+1
    subject=np.append(subject,([d_train[i]]),axis=0)
    #print(image_no%test_subjectimages)
    if(image_no%test_subjectimages==0):#checking if a class is loaded
      print("class")
      #Calculating mean of each class and appending it to classmeans vector
      subject_mean=np.mean(subject,axis=0)
      #subject_mean.reshap
      rows = len(subject_mean)
      # print(subject_mean)
      classmeans = np.append(classmeans, ([subject_mean]),axis=0)
      #subject_mean=classmeans
      #Calculating class scatter
      bracket1=classmeans-mean_vector
      #print((bracket1*bracket1.T).shape)
      sb+=5*np.dot(bracket1.T,bracket1)
      #Centralizing each class (subject)
      z_train=subject-subject_mean
      z_train_trans=np.transpose(z_train)
      #Calculating class scatter matrix
      s+=np.dot(z_train_trans,z_train)
      #Deleting arrays to be used again in loops
      subject = np.empty((0, 10304), float)
      z_train = np.empty((0, 10304), float)
      classmeans=np.empty((0, 10304), float)
      #print(sb)
  sinv=LA.inv(s)
  eigen_values_lda, eigen_vectors_lda = LA.eigh(np.dot(sinv,sb))
  print("Class Mean matrix dimensions")
  print(classmeans.shape)

  print("Between Class scatter Matrix (Sb) dimensions ")
  print(sb.shape)
  print("Within scatter class matrix (s) dimensions")
  print(s.shape)
  #take real values from eigen vectors
  #real values used in eigen vectors (??)
  eigen_vectors_lda_real=eigen_vectors_lda.real
  eigen_values_lda_real=eigen_values_lda.real
  ##id = eigen_values_lda_real.argsort()[::-1]
  #eigen_values_lda_real_sorted = eigen_values_lda_real[id]
  #eigen_vectors_lda_real_sorted = eigen_vectors_lda_real[:,id]
  #print(eigen_vectors_lda_real_sorted)
  #print(eigen_values_lda_real_sorted)
  projection_U39 = np.zeros((10304,39))
  projection_U39 = eigen_vectors_lda_real[:,:39]
  print("Projection Matrix U39: \n",projection_U39)
  projected_data = np.dot(d_train,projection_U39)
  projected_data_test = np.dot(d_test,projection_U39)
  projection_U39.T.shape
  return projected_data, projected_data_test

projected_data, projected_data_test=LDA(d_test, d_train,test_subjectimages)

"""c. Use a simple classifier (first Nearest Neighbor to determine the class labels)"""

accuracy_arr[0]=KNN(1, projected_data, projected_data_test, y_train, y_test)

"""6. Classifier Tuning (20 Points) for LDA:"""

#number of neighbors = 3
accuracy_arr[1]=KNN(3, projected_data, projected_data_test, y_train, y_test)

#number of neighbors = 5
accuracy_arr[2]=KNN(5, projected_data, projected_data_test, y_train, y_test)

#number of neighbors = 7
accuracy_arr[3]=KNN(7, projected_data, projected_data_test, y_train, y_test)

"""For one neighbour classification LDA resulted in higher accuracy than PCA. But when the number of neighbours increased, PCA had higher accuracy.

plot the performance measure
"""

k_values=np.array([1,3,5,7])
plt.plot(k_values,accuracy_arr)
plt.xlabel("k values")
plt.ylabel("accuracy")
plt.title('Performance Measure')
plt.show()

"""Faces vs Non-Faces Using PCA & LDA

"""

#read non face images and stack them into one matrix
d_nf, label_y_nf = Generate("/content/drive/MyDrive/ORL_dataset1/",2,100,200)
d_test_nf, y_test_nf, d_train_nf, y_train_nf,test_subjectimages_nf=Split(d_nf, label_y_nf,100,1,0)

mean_vector_nf, projected_data_u1_nf, projected_data_u2_nf, projected_data_u3_nf, projected_data_u4_nf, projected_data_test_u1_nf, projected_data_test_u2_nf, projected_data_test_u3_nf, projected_data_test_u4_nf =PCA(d_test_nf, y_test_nf, d_train_nf, y_train_nf,200)

KNN(1,projected_data_u1_nf,projected_data_test_u1_nf, y_train_nf, y_test_nf)

projected_data_nf, projected_data_test_nf=LDA(d_test_nf, d_train_nf,test_subjectimages_nf)

KNN(1, projected_data_nf, projected_data_test_nf, y_train_nf, y_test_nf)

def variable_nf(d_train_nf, y_train_nf,d_test_nf,y_test_nf):
  nf_total=100
  ratio=0.4
  accuracy_arr = np.empty(4,float)
  nf_count_arr = np.empty(0,int)
  nf_count=0
  offset=10
  for i in range(4):
    ratio=0.4-(i*0.1)
    print("ratio is",ratio)
    nf_count=((1-ratio)*nf_total)-50
    X_train, X_rubbish, y_train, y_rubbish = train_test_split(d_train_nf, y_train_nf,test_size=ratio,shuffle=False)
    mean_vector_nf, projected_data_u1_nf, projected_data_u2_nf, projected_data_u3_nf, projected_data_u4_nf, projected_data_test_u1_nf, projected_data_test_u2_nf, projected_data_test_u3_nf, projected_data_test_u4_nf =PCA(d_test_nf, y_test_nf, X_train,X_train,len(X_train)*2)
    accuracy_arr[i]=KNN(1, projected_data_u1_nf, projected_data_test_u1_nf, y_train, y_test_nf)
    print(y_train)
    #nf_count = nf_count + offset
    print(nf_count)
    nf_count_arr = np.append(nf_count_arr, [nf_count], axis=0)
 
  plt.plot(nf_count_arr,accuracy_arr)
  plt.xlabel("Number of Non-Face images")
  plt.ylabel("accuracy")
  plt.title('Performance Measure')
  plt.show()
  print("---------------------------------")
  print(nf_count_arr)

variable_nf(d_train_nf,y_train_nf,d_test_nf,y_test_nf)

"""8. Bonus (5 Points)"""

d2, label_y2 = Generate("/content/drive/MyDrive/ORL_dataset/s",40,10,400)
d_test2, y_test2, d_train2, y_train2,images_subject2 = Split(d,label_y,10,0,0.7)

mean_vector2, projected_data2_u1, projected_data2_u2, projected_data2_u3, projected_data2_u4, projected_data2_test_u1, projected_data2_test_u2, projected_data2_test_u3, projected_data2_test_u4 = PCA(d_test2, y_test2, d_train2, y_train2,400)

KNN(1,projected_data2_u1,projected_data2_test_u1, y_train2, y_test2)
KNN(1,projected_data2_u2,projected_data2_test_u2, y_train2, y_test2)
KNN(1,projected_data2_u3,projected_data2_test_u3, y_train2, y_test2)
KNN(1,projected_data2_u4,projected_data2_test_u4, y_train2, y_test2)

"""Using 50% split resultied in lower accuracy than using 70% training and 30% testing. """